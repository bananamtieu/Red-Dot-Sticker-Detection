# -*- coding: utf-8 -*-
"""Red-StickerDetection(TensorFlow).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Odkl8bubMvJJEVwJQxzz9GNVHP3vtHT3
"""

from google.colab import drive
drive.mount('/content/drive')

import json
import requests
import os
from tqdm import tqdm

# Define the path to the NDJSON file
ndjson_file = '/content/drive/MyDrive/RedDotSticker/Red_dot_125.ndjson'
#ndjson_file = 'data.ndjson'
# Directory to save downloaded images
images_dir = 'red_dot_images'
# Output file for structured data
output_file = 'structured_data.json'

# Ensure the images directory exists
os.makedirs(images_dir, exist_ok=True)

structured_data = []

# Function to download images
def download_image(image_url, save_path):
    try:
        response = requests.get(image_url, stream=True)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
        else:
            print(f"Failed to download {image_url}")
    except Exception as e:
        print(f"Error downloading {image_url}: {e}")

# Read the NDJSON file
with open(ndjson_file, 'r') as f:
    for line in tqdm(f, desc="Processing entries"):
        data = json.loads(line)

        # Extract image URL and ID
        image_url = data['data_row']['row_data']
        image_id = data['data_row']['external_id']

        # Download the image (optional)
        image_path = os.path.join(images_dir, image_id)
        download_image(image_url, image_path)

        # Extract bounding box annotation
        project_data = data.get('projects', {})
        for project_id, project_details in project_data.items():
            for label in project_details.get('labels', []):
                for obj in label['annotations']['objects']:
                    # Extract bounding box coordinates
                    bbox = obj['bounding_box']
                    bbox_top = bbox['top']
                    bbox_left = bbox['left']
                    bbox_width = bbox['width']
                    bbox_height = bbox['height']

                    # Append to structured data
                    structured_data.append({
                        'image_id': image_id,
                        'image_path': image_path,
                        'bounding_box': {
                            'top': bbox_top,
                            'left': bbox_left,
                            'width': bbox_width,
                            'height': bbox_height
                        }
                    })

# Save the structured data to a JSON file
with open(output_file, 'w') as out_file:
    json.dump(structured_data, out_file, indent=4)

print()
print(f"Structured data saved to {output_file}")

import json
import tensorflow as tf
import cv2
import numpy as np
from sklearn.model_selection import train_test_split  # Import train_test_split

# Load JSON data
with open('structured_data.json', 'r') as f:
    data = json.load(f)

# Function to rotate image and bounding box
def rotate_image_and_bbox(image, bbox, angle):
    h, w, _ = image.shape

    if angle == 90:
        rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
        rotated_bbox = [
            1 - bbox[7], bbox[6], 1 - bbox[1], bbox[0], 1 - bbox[3], bbox[2], 1 - bbox[5], bbox[4]
        ]  # Apply clockwise 90-degree rotation to all 4 points
    elif angle == 180:
        rotated_image = cv2.rotate(image, cv2.ROTATE_180)
        rotated_bbox = [
            1 - bbox[4], 1 - bbox[5], 1 - bbox[6], 1 - bbox[7], 1 - bbox[0], 1 - bbox[1], 1 - bbox[2], 1 - bbox[3]
        ]  # Apply 180-degree rotation to all 4 points
    elif angle == 270:
        rotated_image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)
        rotated_bbox = [
            bbox[3], 1 - bbox[2], bbox[5], 1 - bbox[4], bbox[7], 1 - bbox[6], bbox[1], 1 - bbox[0]
        ]  # Apply counterclockwise 90-degree rotation to all 4 points
    else:
        return image, bbox

    return rotated_image, rotated_bbox

# Function to load and preprocess image
def load_image_and_bbox(entry):
    img_path = entry['image_path']
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img_resized = cv2.resize(img, (224, 224))  # Resize image to 224x224

    # Normalize the bounding box
    h, w, _ = img.shape
    bbox = entry['bounding_box']
    x1 = bbox['left'] / w
    y1 = bbox['top'] / h
    x2 = (bbox['left'] + bbox['width']) / w
    y2 = bbox['top'] / h
    x3 = (bbox['left'] + bbox['width']) / w
    y3 = (bbox['top'] + bbox['height']) / h
    x4 = bbox['left'] / w
    y4 = (bbox['top'] + bbox['height']) / h
    bbox_normalized = [x1, y1, x2, y2, x3, y3, x4, y4]
    return img_resized, bbox_normalized

# Load images and bounding boxes into arrays
images = []
bounding_boxes = []
for entry in data:
    img, bbox = load_image_and_bbox(entry)
    images.append(img)
    bounding_boxes.append(bbox)

    # Add rotated versions (90, 180, 270 degrees)
    for angle in [90, 180, 270]:
        rotated_img, rotated_bbox = rotate_image_and_bbox(img, bbox, angle)
        images.append(rotated_img)
        bounding_boxes.append(rotated_bbox)

# Convert to NumPy arrays
images = np.array(images) / 255.0  # Normalize image pixel values

bounding_boxes = np.array(bounding_boxes)

# Split the data into training and testing sets
train_images, test_images, train_bboxes, test_bboxes = train_test_split(
    images, bounding_boxes, test_size=0.2, random_state=42  # 20% of data for testing
)

# Print out the shapes to verify the split
print("Training images shape:", train_images.shape)
print("Testing images shape:", test_images.shape)
print("Training bounding boxes shape:", train_bboxes.shape)
print("Testing bounding boxes shape:", test_bboxes.shape)

# Custom MSE loss for 4-point bounding box (x1, y1, x2, y2, x3, y3, x4, y4)
def mse_4_point_loss(y_true, y_pred):
    # Compute MSE for each coordinate
    loss = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)
    return tf.reduce_mean(loss)

from tensorflow.keras import layers, models

# Define the CNN model (no change here, just predicting 4 values)
def create_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(8)  # Output 8 coordinates for the four corners
    ])
    model.compile(optimizer='adam', loss=mse_4_point_loss)
    return model

model = create_model()

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(train_images, train_bboxes, epochs=100, batch_size=4, validation_data=(test_images, test_bboxes),
                    callbacks=[early_stopping])

import matplotlib.pyplot as plt

def getLeftTopWidthHeight(image, bbox):
    bbox = [
        bbox[0] * image.shape[1],  # x1/x_left
        bbox[1] * image.shape[0],  # y1/y_top
        bbox[2] * image.shape[1],  # x2/x_right
        bbox[3] * image.shape[0],  # y2/y_top
        bbox[4] * image.shape[1],  # x3/x_right
        bbox[5] * image.shape[0],  # y3/y_bottom
        bbox[6] * image.shape[1],  # x4/x_left
        bbox[7] * image.shape[0]   # y4/y_bottom
    ]
    x_left = (bbox[0] + bbox[6])/2
    y_top = (bbox[1] + bbox[3])/2
    x_right = (bbox[2] + bbox[4])/2
    y_bottom = (bbox[5] + bbox[7])/2
    bbox_w = x_right - x_left
    bbox_h = y_bottom - y_top
    return x_left, y_top, bbox_w, bbox_h

# Function to draw bounding box using (left, top, width, height)
def draw_bbox(image, bbox_test, bbox_pred):
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    # Denormalize the bounding box coordinates (convert relative values to absolute pixel values)
    left_test, top_test, w_test, h_test = getLeftTopWidthHeight(image, bbox_test)
    rect_test = plt.Rectangle((left_test, top_test), w_test, h_test, linewidth=2, edgecolor='b', facecolor='none')
    ax.add_patch(rect_test)

    left_pred, top_pred, w_pred, h_pred = getLeftTopWidthHeight(image, bbox_pred)
    rect_pred = plt.Rectangle((left_pred, top_pred), w_pred, h_pred, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect_pred)
    plt.show()

# Iterate over test images and display predicted bounding boxes
for i in range(len(test_images)):
    sample_image = test_images[i:i+1]  # Take one test image at a time
    predicted_bbox = model.predict(sample_image)

    # Show the true and predicted bounding box
    print(f"Image {i+1}:")
    print(f"True bounding box: {test_bboxes[i]}")
    print(f"Predicted bounding box: {predicted_bbox[0]}")

    # Display the image with the predicted bounding box
    draw_bbox(test_images[i], test_bboxes[i], predicted_bbox[0])
